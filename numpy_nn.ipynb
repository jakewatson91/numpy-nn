{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a two-layer Neural Net in Numpy using the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load MNIST datasets from:\n",
    "https://s3.amazonaws.com/jrwprojects/fashion_mnist_train_images.npy\n",
    "https://s3.amazonaws.com/jrwprojects/fashion_mnist_train_labels.npy\n",
    "https://s3.amazonaws.com/jrwprojects/fashion_mnist_test_images.npy\n",
    "https://s3.amazonaws.com/jrwprojects/fashion_mnist_test_labels.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('data/fashion_mnist_train_images.npy')\n",
    "y_train = np.load('data/fashion_mnist_train_labels.npy')\n",
    "\n",
    "X_test = np.load('data/fashion_mnist_test_images.npy')\n",
    "y_test = np.load('data/fashion_mnist_test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n",
      "(48000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "X_train_scaled = x_scaler.fit_transform(X_train)\n",
    "X_val_scaled = x_scaler.transform(X_val)\n",
    "X_test_scaled = x_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Numerical stability improvement\n",
    "        return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "class NN():\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        np.random.seed(91)\n",
    "        self.w1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "\n",
    "        self.w2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w1) + self.b1 # shape (batch_size, hidden_size)\n",
    "        a = relu(z)\n",
    "        z2 = np.dot(a, self.w2) + self.b2 # shape (batch_size, output_size)\n",
    "        y_pred = softmax(z2)\n",
    "        return z, a, z2, y_pred # shape (batch_size, output_size)\n",
    "    \n",
    "    def loss_fn(self, batch_size, y, y_pred, alpha):\n",
    "        predicted_probs = y_pred[range(batch_size), y]\n",
    "        log_likelihood = -np.log(predicted_probs)\n",
    "        loss = np.sum(log_likelihood) / batch_size\n",
    "\n",
    "        l2 = (alpha / 2) * (np.sum(self.w1**2) + np.sum(self.w2**2))\n",
    "        loss += l2\n",
    "        return loss\n",
    "    \n",
    "    def backprop(self, batch_size, y, y_pred, x, z, a, lr):\n",
    "        y_onehot = np.zeros_like(y_pred)\n",
    "        y_onehot[np.arange(batch_size), y] = 1\n",
    "\n",
    "        # gradients\n",
    "        d_z2 = (y_pred - y_onehot) / y_pred.shape[0]\n",
    "        d_w2 = a.T @ d_z2  # Gradient of W2\n",
    "        d_b2 = np.sum(d_z2, axis=0, keepdims=True)  # Gradient of b2\n",
    "        d_a = d_z2 @ self.w2.T  # Backpropagate through W2\n",
    "        d_z1 = d_a * (z > 0)  # Backpropagate through ReLU (derivative of ReLU)\n",
    "        d_w1 = x.T @ d_z1  # Gradient of W1\n",
    "        d_b1 = np.sum(d_z1, axis=0, keepdims=True)  # Gradient of b1\n",
    "\n",
    "        # Gradient descent update (with learning rate)\n",
    "        self.w1 -= lr * d_w1\n",
    "        self.b1 -= lr * d_b1\n",
    "        self.w2 -= lr * d_w2\n",
    "        self.b2 -= lr * d_b2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train_scaled.shape[1]\n",
    "hidden_size = 40\n",
    "output_size = 10 # 10 class labels\n",
    "alpha = 0.1\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "lr = 0.0002\n",
    "\n",
    "model = NN(input_size, hidden_size, output_size)\n",
    "\n",
    "def train(X, y, epochs, num_samples):\n",
    "    total_loss = 0\n",
    "    for _ in range(epochs):\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch = indices[i : i + batch_size]\n",
    "            X_batch, y_batch = X[batch], y[batch]\n",
    "\n",
    "            cur_batch_size = len(X_batch)\n",
    "            z, a, _, y_pred = model.forward(X_batch)\n",
    "            loss = model.loss_fn(cur_batch_size, y_batch, y_pred, alpha)\n",
    "            model.backprop(cur_batch_size, y_batch, y_pred, X_batch, z, a, lr)\n",
    "            epoch_loss += loss\n",
    "        epoch_loss = epoch_loss / (num_samples // batch_size) # num batches\n",
    "        # print(f\"Epoch loss: {epoch_loss}\")\n",
    "        total_loss += epoch_loss\n",
    "    # print(f\"Avg_loss: {total_loss / epochs}\") \n",
    "\n",
    "def val(X, y, num_samples):\n",
    "    total_loss = 0\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        X_batch = X[i : i + batch_size]\n",
    "        y_batch = y[i : i + batch_size]\n",
    "        cur_batch_size = len(X_batch)\n",
    "\n",
    "        _, _, _, y_pred = model.forward(X_batch)\n",
    "        prediction = np.argmax(y_pred, axis=1)\n",
    "        loss = model.loss_fn(cur_batch_size, y_batch, y_pred, alpha)\n",
    "        total_loss += (loss / (num_samples // batch_size))\n",
    "        accuracy = np.sum(prediction == y_batch) / batch_size\n",
    "    # print(f\"Val loss: {total_loss}\")\n",
    "    # print(f\"Accuracy: {accuracy}\")\n",
    "    return total_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 2.4591702298250753\n",
      "Epoch loss: 2.45752670747476\n",
      "Epoch loss: 2.45566420908833\n",
      "Epoch loss: 2.4534551569578116\n",
      "Epoch loss: 2.4507302094204086\n",
      "Epoch loss: 2.447303064011403\n",
      "Epoch loss: 2.442968021391681\n",
      "Epoch loss: 2.43748816391725\n",
      "Epoch loss: 2.430606421757479\n",
      "Epoch loss: 2.422077535169948\n",
      "Avg_loss: 2.4456989719014146\n",
      "Total loss: 2.429888749565764\n"
     ]
    }
   ],
   "source": [
    "train(X_train_scaled, y_train, num_samples = X_train_scaled.shape[0])\n",
    "val(X_val_scaled, y_val, num_samples = X_val_scaled.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current best params: ('Batch size: 32', 'Learning rate: 0.001', 'Epochs: 5', 'Alpha: 0.001', 'Hidden size: 20')\n",
      "\n",
      "Loss: 1.9323, Accuracy: 0.2188\n",
      "\n",
      "Current best params: ('Batch size: 32', 'Learning rate: 0.001', 'Epochs: 5', 'Alpha: 0.001', 'Hidden size: 40')\n",
      "\n",
      "Loss: 1.8051, Accuracy: 0.2500\n",
      "\n",
      "Current best params: ('Batch size: 32', 'Learning rate: 0.001', 'Epochs: 10', 'Alpha: 0.001', 'Hidden size: 20')\n",
      "\n",
      "Loss: 1.2309, Accuracy: 0.2812\n",
      "\n",
      "Current best params: ('Batch size: 32', 'Learning rate: 0.001', 'Epochs: 10', 'Alpha: 0.001', 'Hidden size: 40')\n",
      "\n",
      "Loss: 1.1725, Accuracy: 0.2969\n",
      "\n",
      "Current best params: ('Batch size: 32', 'Learning rate: 0.001', 'Epochs: 20', 'Alpha: 0.001', 'Hidden size: 20')\n",
      "\n",
      "Loss: 0.8441, Accuracy: 0.3125\n",
      "\n",
      "Current best params: ('Batch size: 32', 'Learning rate: 0.001', 'Epochs: 20', 'Alpha: 0.001', 'Hidden size: 40')\n",
      "\n",
      "Loss: 0.8233, Accuracy: 0.3125\n",
      "\n",
      "Current best params: ('Batch size: 64', 'Learning rate: 0.001', 'Epochs: 20', 'Alpha: 0.001', 'Hidden size: 40')\n",
      "\n",
      "Loss: 0.8233, Accuracy: 0.3125\n",
      "\n",
      "Current best params: ('Batch size: 128', 'Learning rate: 0.001', 'Epochs: 20', 'Alpha: 0.001', 'Hidden size: 40')\n",
      "\n",
      "Loss: 0.8233, Accuracy: 0.3125\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "mini_batches = [32, 64, 128]\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "epochs = [5, 10, 20]\n",
    "l2_alphas = [1e-3, 1.0]\n",
    "hidden_size = [20, 40]\n",
    "\n",
    "best_params = {}\n",
    "loss = np.inf\n",
    "\n",
    "for b, lr, e, l2, h in product(mini_batches, learning_rates, epochs, l2_alphas, hidden_size):\n",
    "    alpha = l2 \n",
    "    lr = lr\n",
    "    model = NN(input_size, h, output_size)\n",
    "    train(X_train_scaled, y_train, e, num_samples = X_train_scaled.shape[0])\n",
    "    val_loss, accuracy = val(X_val_scaled, y_val, num_samples = X_val_scaled.shape[0])\n",
    "    if val_loss <= min(val_loss, loss):\n",
    "        loss = val_loss\n",
    "        best_params = {\"Batch size\" : b, \"Learning rate\" : lr, \"Epochs\" : e, \"Alpha\" : l2, \"Hidden size\" : h}\n",
    "        print(f\"\\nCurrent best params: {best_params}\")\n",
    "        print(f\"\\nLoss: {val_loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Best params: {best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs541",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
